{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de03d8-a8e5-4e85-b3bd-072af2198794",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "!pip install ucimlrepo\n",
    "!pip install mlxtend\n",
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7e93d-4fdf-42ef-9d57-b06c4515c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv()\n",
    "data.info()\n",
    "data.drop([], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04736fde-f33e-4982-af49-5865d436f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = data.select_dtypes(include = ['int64', 'float64']).columns.tolist()\n",
    "numerical_columns\n",
    "data[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38c375-e953-4d03-b509-12e53f164cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "sc = StandardScaler()\n",
    "data_scaled = sc.fit_transform(data)\n",
    "\n",
    "# epsilon value\n",
    "neighbor = NearestNeighbors(n_neighbors = 5)\n",
    "neighbordist = neighbor.fit(data_scaled)\n",
    "distance, indices = neighbordist.kneighbors(data_scaled)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e857ad8-1018-40ad-be8a-8fa463187edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "distance = np.sort(distance, axis = 0)\n",
    "plt.plot(distance[:, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa096773-9536-43d6-afb2-514cac46423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [0.6, 0.7, 0.8, 0.9]\n",
    "min_pts = [4, 5, 6] #2 * no. of dimention i.e. 2*3 = 6 to be chosen\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "result = []\n",
    "for e in eps:\n",
    "    for n in min_pts:\n",
    "        dbscan = DBSCAN(eps = e, min_samples = n)\n",
    "        y_cluster = dbscan.fit_predict(data_scaled)\n",
    "        n_cluster = len(set(y_cluster)) - (1 if -1 in y_cluster else 0)\n",
    "        n_noise = list(y_cluster).count(-1)\n",
    "        result.append((e, n, n_cluster, n_noise))\n",
    "\n",
    "result_df = pd.DataFrame(result, columns = ['eps', 'minsamples', 'n_clusters', 'n_noise'])\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1236513-8480-41d1-aff5-e53302baff7a",
   "metadata": {},
   "source": [
    "WHAT IS LEARNING?\n",
    "Ability to improve once behavior with experience\n",
    "WHAT IS MACHINE LEARNING?\n",
    "Explores algorithms that learn from Data, build models from Data and this model can be used for different tasks. Eg) Prediction, Decision making or solving tasks\n",
    "Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed\n",
    "\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.“\n",
    "•TASKS – T – behavior of the task that the learning program is seeking to improve. Like Prediction, Classification and acting in an environment\n",
    "•Experience –E- Data: Used for improving the experience at the task\n",
    "•Measure of improvement P – Increase accuracy or new skills to agents or increase efficiency of problem solving we can improve the performance measure\n",
    "•Learner: Gains experience from data and incorporates background knowledge.\n",
    "•Reasoner: Solves tasks or problems based on models built by the learner and delivers solutions or performance.\n",
    "Key Elements:\n",
    "\n",
    "    Experience on Data: Input from real-world data used for learning.\n",
    "    Problem or Task: The challenge that needs solving, driving the reasoning process.\n",
    "    Background Knowledge: Pre-existing knowledge that aids the learning process.\n",
    "    Build Models: Collaboration between the learner and reasoner to produce models.\n",
    "    Answer or Solution Performance: The measurable outcome of the reasoning process.\n",
    "\n",
    "This framework is widely applied in fields like Machine Learning, Artificial Intelligence, and Knowledge-Based Systems, emphasizing how models are developed, learned, and used for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993ad20-5eb1-4453-8285-bc0eac2c9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model = DBSCAN(eps = 0.6, min_samples = 6)\n",
    "y_cluster = db_model.fit_predict(data_scaled)\n",
    "\n",
    "data['Clusters'] = y_cluster\n",
    "cluster_analysis = data.groupby('Clusters').mean()\n",
    "cluster_analysis\n",
    "\n",
    "#with noise data points to identify outlier\n",
    "plt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'],\n",
    "            c = data['Clusters'], cmap = 'viridis')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('DBSCAN for Mall Customer')\n",
    "plt.show()\n",
    "\n",
    "#without noise data points\n",
    "data = data[data['Clusters']!=-1]\n",
    "plt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'],\n",
    "            c = data['Clusters'], cmap = 'viridis')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('DBSCAN for Mall Customer')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "ss_score = silhouette_score(data_scaled, y_cluster)\n",
    "dv_score = davies_bouldin_score(data_scaled, y_cluster)\n",
    "ch_score = calinski_harabasz_score(data_scaled, y_cluster)\n",
    "print('Silhouette Score', ss_score)\n",
    "print('Davis Bouldin', dv_score)\n",
    "print('Calinski Score', ch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff8049-fb1b-4bed-90a0-0f0805c25f66",
   "metadata": {},
   "source": [
    "MACHINE LEARNING VS TRADITIONAL PROGRAMMING\n",
    "I. Approach to Problem-Solving:\n",
    "•Traditional Programming:\n",
    "•In traditional programming, a programmer writes code (rules) to solve a problem or perform a task.\n",
    "•The input and the code produce the output.\n",
    "•For instance, in a calculator program, the programmer explicitly codes all functionalities (like addition, subtraction) needed for the calculations.\n",
    "\n",
    "•Machine Learning:\n",
    "•In machine learning, the input (data) and the output (answers) are fed into an algorithm to create a model.\n",
    "•The model, once trained, can take new inputs to predict or make decisions based on its learning.\n",
    "•For example, in a machine learning model trained to recognize handwritten digits, the model learns from a dataset of digits and their labels.\n",
    "\n",
    "II Data Dependency:\n",
    "\n",
    "Traditional Programming:\n",
    "It doesn't rely heavily on data.\n",
    "The focus is more on logic and algorithms that don't change unless manually updated.\n",
    "\n",
    "Machine Learning:\n",
    "It is heavily dependent on data.\n",
    "The quality and quantity of data fed into the model significantly affect its accuracy and efficiency.\n",
    "\n",
    "III Flexibility and Adaptability:\n",
    "•Traditional Programming:\n",
    "•Changes in problem specifications or requirements necessitate changes in the program's code.\n",
    "•The system does not adapt automatically to new scenarios.\n",
    "\n",
    "•Machine Learning:\n",
    "•Models can adapt to new data independently.\n",
    "•They can learn from new data and improve over time, making them more flexible in handling unforeseen scenarios.\n",
    "\n",
    "IV Complexity of Problems Solved:\n",
    "\n",
    "•Traditional Programming:\n",
    "•More effective for problems with clear rules and logic.\n",
    "•Struggles with tasks that are too complex to manually code, like natural language processing or image recognition.\n",
    "\n",
    "•Machine Learning:\n",
    "•Excels at handling complex problems that are difficult to solve with traditional rule-based programming, especially when patterns or correlations are not immediately apparent.\n",
    "\n",
    "V Output Predictability:\n",
    "•\n",
    "• Traditional Programming:\n",
    "•Outputs are predictable and consistent for the same input and code.\n",
    "\n",
    "•Machine Learning:\n",
    "•Outputs can vary, and there is a probability of error or inaccuracy, especially in new or edge-case scenarios.\n",
    "\n",
    "1. Supervised learning – Also called predictive learning.\n",
    "•A machine predicts the class of unknown objects based on prior class-related information of similar objects.\n",
    "2. Unsupervised learning – Also called descriptive learning.\n",
    "•A machine finds patterns in unknown objects by grouping similar objects together.\n",
    "3. Reinforcement learning – Also called as Trial and Error Learning\n",
    "•A machine learns to act on its own to achieve the given goals.\n",
    "\n",
    "•The training set given for supervised learning is the labeled dataset.\n",
    "•Supervised learning tries to find the relationships between the feature set and the label set, which is the knowledge and properties we can learn from labeled dataset.\n",
    "• If each feature vector x is corresponding to a label y e L, L = {l1, l1, ... lc}\n",
    "•(c is usually ranged from 2 to a hundred), the learning problem is denoted as classification. On the other hand, if each feature vector x is corresponding to a real value y ЄR Î , the learning problem is defined as regression problem.\n",
    "•The knowledge extracted from supervised learning is often utilized for prediction and recognition\n",
    "\n",
    "Objective:\n",
    "To evaluate a machine learning model's performance on a limited dataset by dividing it into multiple folds for training and testing.\n",
    "Dataset Split:\n",
    "       The dataset is split into two parts:\n",
    "        Training subset: Used for building models.\n",
    "        Test subset: Used for final evaluation.\n",
    "k-Fold Process:\n",
    "   The training subset is further divided into k equally sized folds (subsets).\n",
    "    For each of the k runs (iterations):\n",
    "        1 fold is used as the Test fold for validation.\n",
    "        The remaining k-1 folds are used as Training folds to build the model.\n",
    "\n",
    "Steps in Each Run:\n",
    "    A model is trained on k−1.\n",
    "    It is validated (performance measured) on the Test fold.\n",
    "    The process is repeated k times, each time using a different fold as the test set.\n",
    "\n",
    "Scores:\n",
    "    After each run, the model’s performance score S1,S2,...,Sk is recorded.\n",
    "    The final performance of the model is calculated as the average score across all k runs.\n",
    "\n",
    "Model Selection and Evaluation:\n",
    "    The model with the best performance metric is selected.\n",
    "    The final model is built on the full training subset.\n",
    "    The selected model is evaluated on the test subset to estimate the generalization error.\n",
    "\n",
    "SUPERVISED LEARNING EXAMPLES\n",
    "•Supervised learning is effective for a variety of business purposes, including sales forecasting, inventory optimization, and fraud detection. Some examples of use cases include:\n",
    "•Predicting real estate prices\n",
    "•Classifying whether bank transactions are fraudulent or not\n",
    "•Finding disease risk factors\n",
    "•Determining whether loan applicants are low-risk or high-risk\n",
    "•Predicting the failure of industrial equipment's mechanical parts\n",
    "\n",
    "UNSUPERVISED LEARNING ALGORITHM\n",
    "•The training set given for unsupervised leaning is the unlabeled dataset.\n",
    "• Unsupervised learning aims at clustering, probability density estimation, finding association among features, and dimensionality reduction.\n",
    "• In general, an unsupervised algorithm may simultaneously learn more than one properties listed above, and the results from unsupervised learning could be further used for supervised learning.\n",
    "\n",
    "Unsupervised learning examples\n",
    "•Unsupervised algorithms are widely used to create descriptive models.\n",
    "•Common applications also include clustering, which creates a model that groups objects together based on specific properties, and association, which identifies the rules between the clusters.\n",
    "•A few example use cases include:\n",
    "•Creating customer groups based on purchase behavior\n",
    "•Grouping inventory according to sales and/or manufacturing metrics\n",
    "•Pinpointing associations in customer data (for example, customers who buy a specific style of handbag might be interested in a specific style of shoe)\n",
    "\n",
    "REINFORCEMENT LEARNING ALGORITHM\n",
    "\n",
    "•RL is a machine learning paradigm where an agent learns from rewards obtained by performing a series of actions.\n",
    "•The agent does not receive explicit instructions or correct/false labels for its actions.\n",
    "•Feedback comes in the form of rewards, and the agent must explore and discover the optimal actions to achieve maximum rewards.\n",
    "•Typical applications of reinforcement learning involve playing games (chess, Go, Atari video games) and some form of robots, e.g., drones, warehouse robots, and more recently self driving cars.\n",
    "•Robotics: Training robots to perform tasks through trial and error.\n",
    "•Game AI: Teaching agents to play games like chess or Go.\n",
    "•Autonomous Vehicles: Learning to navigate environments safely and efficiently.\n",
    "•Learning by interacting with an environment to maximize rewards.\n",
    "•Example: Training a robot to walk or play chess.\n",
    "•Common approaches: Q-Learning, Deep Q-Networks (DQN).\n",
    "\n",
    "•Reinforcement learning examples\n",
    "Practical applications for this type of machine learning are still emerging. Some examples of uses include:\n",
    "•Teaching cars to park themselves and drive autonomously\n",
    "•Dynamically controlling traffic lights to reduce traffic jams\n",
    "•Training robots to learn policies using raw video images as input that they can use to replicate the actions they see\n",
    "•\n",
    "\n",
    "SEMI SUPERVISED LEARNING\n",
    "•In addition to these three types, a fourth type of machine learning category, semi-supervised learning, has attracted increasing attention recently.\n",
    "•It is defined between supervised and unsupervised learning, contains both labeled and unlabeled data, and jointly learns knowledge from them.\n",
    "\n",
    "HYPOTHESIS SPACE and INDUCTIVE BIAS\n",
    "•In inductive learning or prediction problems:\n",
    "•Given examples / data\n",
    "•Examples are of the form (x,y)\n",
    "•x is the instances and y is the output\n",
    "•This can be specified as (x,f(x)). We want to learn x\n",
    "•For a classification problem:\n",
    "•Classification – f(x): discrete\n",
    "•Regression – f(x): continuous\n",
    "•Probability estimations – f(x): probability (x)\n",
    "\n",
    "FEATURE SPACE\n",
    "•When we say we have to learn a function, it is a function of the features; so instances are described in terms of features.\n",
    "•Features are properties that describe each instance.\n",
    "•Each instance can be described in a quantitative manner using features.\n",
    "•Often we have multiple features so we have what we call a feature vector, for example, for a particular instance we may be or a particular task we may be describing all the instances in terms of ten features, so the feature vector will be a one-dimensional vector of size 10.\n",
    "•\n",
    "•\n",
    "\n",
    "APPLICATIONS OF MACHINE LEARNING\n",
    "•After the field of machine learning was “founded” more than a half a century ago, we can now find applications of machine learning in almost every aspect of our life.\n",
    "•Popular applications of machine learning include the following:\n",
    "• Email spam detection\n",
    "• Face detection and matching (e.g., iPhone X, Windows laptops, etc.)\n",
    "• Web search (e.g., DuckDuckGo, Bing, Baidu, Google)\n",
    "• Sports predictions\n",
    "•\n",
    "• Post office (e.g., sorting letters by zip codes)\n",
    "• ATMs (e.g., reading checks)\n",
    "• Credit card fraud\n",
    "• Stock predictions\n",
    "• Smart assistants (Apple Siri, Amazon Alexa, . . . )\n",
    "• Product recommendations (e.g., Walmart, Netflix, Amazon)\n",
    "• Self-driving cars (e.g., Uber, Tesla)\n",
    "• Language translation (Google translate)\n",
    "• Sentiment analysis\n",
    "•Drug design\n",
    "• Medical diagnoses\n",
    "\n",
    "DOMAINS AND APPLICATIONS\n",
    "MEDICINE:\n",
    "•DIAGNOSE DISEASE\n",
    "•Input symptoms, lab measurements, test results, DNA tests,….\n",
    "•Output: one of set of possible disease or “none of the above”\n",
    "•Data mine historical medical records to learn which future patients will respond best to which treatments.\n",
    "VISION:\n",
    "•Say what objects appear in an image\n",
    "•Convert hand-written digits to characters 0…9\n",
    "•Detect where objects appear in an image\n",
    "ROBOT CONTROL:\n",
    "•Design autonomous mobile robots that learn to navigate from their own experience.\n",
    "NLP:\n",
    "•Detect where entities are mentioned in NL\n",
    "•Detect what facts are expressed in NL\n",
    "•Detect if a product/movie review is positive, negative or neutral\n",
    "•\n",
    "•Speech Recognition\n",
    "•Machine translation\n",
    "Financial:\n",
    "•Predict is a stock will rise or fall\n",
    "•In the next few milliseconds\n",
    "•Predict if a user will click on an ad or not\n",
    "•In order to decide which ad to show\n",
    "•\n",
    "•\n",
    "\n",
    "APPLICATION IN BUSINESS INTELLIGENCE\n",
    "•Forecasting product sales, quantities taking seasonality and trend into account\n",
    "•Identifying cross selling promotional opportunities for consumer goods\n",
    "•Identify the price sensitivity of a consumer product and identify the optimum price point that maximizes net profit\n",
    "•Optimizing product location at a super market retail outlet.\n",
    "•Modeling variables impacting customers churn and refining strategy.\n",
    "•\n",
    "•\n",
    "OTHER APPLICATIONS\n",
    "•Fraud Detection: Credit card providers\n",
    "•Determine whether or not someone will default on a home mortgage\n",
    "•Understand consumer sentiment based on unstructured data\n",
    "•Forecasting women’s conviction rates based on external macroeconomic factors\n",
    "•\n",
    "\n",
    "HOW TO CREATE A LEARNER?\n",
    "1.Choose the training experience or the data. It is nothing but the features\n",
    "2.Choose the target function on how we want to represent the model. Ie.) to be learned.\n",
    "3.Choose how to represent the target function . Appropriate class of functions on the features. This class of function is called hypothesis language\n",
    "4.Choose a Learning algorithm to infer the target function.\n",
    "\n",
    "MACHINE LEARNING PIPELINE\n",
    "•A machine learning pipeline is a way to codify and automate the workflow it takes to produce a machine learning model.\n",
    "•Machine learning pipelines consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment.\n",
    "\n",
    "I Data Ingestion: \n",
    "\n",
    "•The process begins with ingesting raw data from different sources, such as databases, files, or APIs.\n",
    "•This step is crucial to ensure that the pipeline has access to relevant and up-to-date information.\n",
    "•\n",
    "\n",
    "II Data Preprocessing: \n",
    "•Raw data often contains noise, missing values, or inconsistencies.\n",
    "•The preprocessing stage involves cleaning, transforming, and encoding the data, making it suitable for machine learning algorithms.\n",
    "•Common preprocessing tasks include handling missing data, normalization, and categorical encoding.\n",
    "•\n",
    "\n",
    "III Feature Engineering: \n",
    "•In this stage, new features are created from the existing data to improve model performance.\n",
    "•Techniques such as dimensionality reduction, feature selection, or feature extraction can be employed to identify and create the most informative features for the ML algorithm.\n",
    "•Business knowledge can come in handy at this step of the pipeline.\n",
    "•\n",
    "\n",
    "IV Model Training: \n",
    "•The preprocessed data is fed into the chosen ML algorithm to train the model.\n",
    "•The training process involves adjusting the model’s parameters to minimize a predefined loss function, which measures the difference between the model’s predictions and the actual values.\n",
    "•\n",
    "\n",
    "V Model Validation:\n",
    "•To evaluate the model’s performance, a validation dataset (a portion of the data that the model never saw) is used.\n",
    "•Metrics such as accuracy, precision, recall, or F1-score can be employed to assess how well the model generalizes to new (unseen data) in classification problems.\n",
    "\n",
    "VI Hyper parameter Tuning:\n",
    "•Hyper parameters are the parameters of the ML algorithm that are not learned during the training process but are set before training begins.\n",
    "•Tuning hyper parameters involves searching for the optimal set of values that minimize the validation error and helps achieve the best possible model’s performance.\n",
    "\n",
    "VII Model evaluation\n",
    "• After training, the model's performance is assessed using a separate testing dataset or through cross-validation.\n",
    "• Common evaluation metrics depend on the specific problem but may include accuracy, precision, recall, F1-score, mean squared error or others.\n",
    "•\n",
    "VIII Model deployment:\n",
    "•Once a satisfactory model is developed and evaluated, it can be deployed to a production environment where it can make predictions on new, unseen data.\n",
    "•Deployment may involve creating APIs and integrating with other systems.\n",
    "\n",
    "IX Monitoring and maintenance:\n",
    "•After deployment, it's important to continuously monitor the model's performance and retrain it as needed to adapt to changing data patterns.\n",
    "•This step ensures that the model remains accurate and reliable in a real-world setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7f217-dac8-48dc-9e87-5a4c772499c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ffd85-9a4a-4c12-ad8f-2f4966f945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f6388-79f4-4ec5-8b67-08a6460d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "scatter = sns.scatterplot(x = data['Annual Income (k$)'], y = data['Spending Score (1-100)'], hue = data['Age'],\n",
    "                          palette = 'viridis', size = data['Age'], sizes = (20, 200))\n",
    "plt.title('Scatterplot between AI vs SS w.r.t Age')\n",
    "plt.xlabel('Annual Income', fontsize = 12)\n",
    "plt.ylabel('Spending Score', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4a58b-9b64-436e-a11c-1bba14724eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "sns.scatterplot(x = 'Annual Income (k$)', y = 'Spending Score (1-100)', data = data, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f10b7d-2b16-4bb3-b16b-e0ce1a557e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data_final = data.drop(['Age', 'Genre'], axis = 1)\n",
    "sc = StandardScaler()\n",
    "data_scaled = sc.fit_transform(data_final)\n",
    "sum_of_squared_distance = []\n",
    "s_score = []\n",
    "k_range = range(2, 10)\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters = k, random_state = 42)\n",
    "    km.fit(data_scaled)\n",
    "    sum_of_squared_distance.append(km.inertia_)\n",
    "    s_score.append(silhouette_score(data_scaled, km.labels_))\n",
    "sum_of_squared_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e77a8-1b9f-434b-9b92-c301f83fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, sum_of_squared_distance, marker = 'o')\n",
    "plt.title('Elbow Method - Inertia')\n",
    "plt.xlabel('No. of clusters')\n",
    "plt.ylabel('SS distance')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, s_score, marker = 'o')\n",
    "plt.title('Elbow Method - SS')\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071aa62-7cb0-4fc5-b2a4-703739986f7f",
   "metadata": {},
   "source": [
    "BENEFITS OF PIPELINE\n",
    "•Unattended runs\n",
    "•Easy Debugging\n",
    "•Easy tracking and versioning\n",
    "•Fast execution\n",
    "•Collaboration\n",
    "•Reusability\n",
    "•Heterogeneous Compute\n",
    "\n",
    "•Benefits of Machine Learning Pipelines\n",
    "•Unattended runs\n",
    "•The pipeline allows to schedule different steps to run in parallel in a reliable and unattended way.\n",
    "•It means you can focus on other tasks simultaneously when the process of data modeling and preparation is going on.\n",
    "\n",
    "•Easy Debugging\n",
    "•Using pipeline, there is a separate function for each task(such as different functions for data cleaning and data modeling).\n",
    "•It becomes easy to debug the complete code and find out the issues in a particular step.\n",
    "\n",
    "•Easy tracking and versioning\n",
    "•We can use a pipeline to explicitly name and version the data sources, inputs, and output rather than manually tracking data and outputs for each iteration.\n",
    "\n",
    "•Fast execution\n",
    "•As we discussed above, in the ML pipeline, each part of the workflow acts as an independent element, which allows the software to run faster and generate an efficient and high-quality output.\n",
    "\n",
    "•Collaboration\n",
    "•Using pipelines, data scientists can collaborate over each phase of the ML design process and can also work on different pipeline steps simultaneously.\n",
    "\n",
    "\n",
    "•Reusability\n",
    "•We can create pipeline templates for particular scenarios and can reuse them as per requirement.\n",
    "•For example, creating a template for retraining and batch scoring.\n",
    "•\n",
    "\n",
    "•Heterogeneous Compute\n",
    "•We can use multiple pipelines which are reliably coordinated over heterogeneous computer resources as well as different storage locations.\n",
    "• It allows making efficient use of resources by running separate pipelines steps on different computing resources, e.g., GPUs, Data Science VMs, etc.\n",
    "•\n",
    "CHALLANGES IN MACHINE LEARNING\n",
    "•Do I have enough data?\n",
    "•Is the data of sufficient quality?\n",
    "•Errors in data. Eg. Age=255 ; noise in low resolution images\n",
    "•Missing values\n",
    "•Am I describing the data correctly?\n",
    "•Are age and income enough? Should I look at Gender also?\n",
    "•How should I represent age? As a number or as young, middle age , old?\n",
    "•How good is a model?\n",
    "•How do I choose a model?\n",
    "•How confident can I be of the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80125abc-16b9-436d-93dc-87cf0c7285e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m visulaizer \u001b[38;5;241m=\u001b[39m KElbowVisualizer(model, k \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m8\u001b[39m), metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhouette\u001b[39m\u001b[38;5;124m'\u001b[39m, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m visulaizer\u001b[38;5;241m.\u001b[39mfit(data_scaled)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "model = KMeans(random_state = 42)\n",
    "visulaizer = KElbowVisualizer(model, k = (2,8), metric = 'silhouette', timings = False)\n",
    "visulaizer.fit(data_scaled)\n",
    "visulaizer.poof()\n",
    "e = visulaizer.elbow_value_\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d12155-d464-4082-be92-7b3485bc2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = e, random_state = 42)\n",
    "y_label = km.fit_predict(data_scaled)\n",
    "data['Clusters'] = y_label\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44e232-4c73-4fb6-805a-3dc384f59b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Centroid\",km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4f98b-bb45-4432-8bd0-f7047d53008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Genre', axis = 1, inplace = True)\n",
    "cluster_analysis=data.groupby('Clusters').mean()\n",
    "cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79231ef-c55a-4d37-b038-c574b1068284",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "df1 = data[data.Clusters == 0]\n",
    "df2 = data[data.Clusters == 1]\n",
    "df3 = data[data.Clusters == 2]\n",
    "df4 = data[data.Clusters == 3]\n",
    "df5 = data[data.Clusters == 4]\n",
    "plt.scatter(df1['Annual Income (k$)'], df1['Spending Score (1-100)'], color = 'orange', label = 'Standard')\n",
    "plt.scatter(df2['Annual Income (k$)'], df2['Spending Score (1-100)'], color = 'magenta', label = 'Careless')\n",
    "plt.scatter(df3['Annual Income (k$)'], df3['Spending Score (1-100)'], color = 'green', label = 'Target Group')\n",
    "plt.scatter(df4['Annual Income (k$)'], df4['Spending Score (1-100)'], color = 'red', label = 'Careful')\n",
    "plt.scatter(df5['Annual Income (k$)'], df5['Spending Score (1-100)'], color = 'blue', label = 'Sensible')\n",
    "plt.title('Cluster Result', fontweight = 'bold', fontsize = 20)\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.legend(fontsize = 15)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec66b3-466c-44d5-92a4-58cf6fdc6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "score_kmeans_ss = silhouette_score(data_scaled, km.labels_)\n",
    "print(score_kmeans_ss)\n",
    "score_kmeans_c = calinski_harabasz_score(data_scaled, km.labels_)\n",
    "print(score_kmeans_c)\n",
    "score_kmeans_d = davies_bouldin_score(data_scaled, km.labels_)\n",
    "print(score_kmeans_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f1c42-b317-4651-954e-77c2543a4c6f",
   "metadata": {},
   "source": [
    "Silhouette Score: Measure how similar an object is within its cluster Value of this silhouette score is between -1 & +1 1 means it is a well seperated cluster 0 means overlapping clusters -1 means poor clustering\n",
    "\n",
    "Calinski-Harabasz Score: 0 & +infinity variance ratio criteria ratio = between cluster dispersion/within cluster dispersion the value is high means better is the clustering\n",
    "\n",
    "Davis Bouldin Score: 0 & +infinity average similarity between each cluster and its most similar cluster intra-cluster similarity and inter cluster differences This value should be lower for better clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
